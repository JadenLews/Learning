{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b325813",
   "metadata": {},
   "source": [
    "Requirements to run:\n",
    "\n",
    "torch, matplotlib, opencv-python, tqdm, numpy\n",
    "\n",
    "train_sims.npy, val_sims.npy, test_sims.npy (You may need to change the paths)\n",
    "\n",
    "Data in 200x200 format with conductivity, pressure, and porosity. (Again, you may need to change path)\n",
    "\n",
    "At the bottom in the evaluation section, there are some pre-named models. You should replace these with the names of your saved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3bf807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SAVE_DIR = \"meta2\"  \n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Prevents crashes when showing graphs\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e5b24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Determined train/test/val split\n",
    "train_sims = np.load(\"../train_sims.npy\")\n",
    "train_sims = train_sims[train_sims < 250]\n",
    "val_sims = np.load(\"../val_sims.npy\")\n",
    "val_sims = val_sims[val_sims < 250]\n",
    "test_sims = np.load(\"../test_sims.npy\")\n",
    "test_sims = test_sims[test_sims < 250]\n",
    "\n",
    "# Folders\n",
    "BINARY_FOLDER = \"../Data200x200_withinfo\"\n",
    "UNIFORM_FOLDER = \"../Uniform200x200withInfo\"\n",
    "\n",
    "# Get porosity phi\n",
    "def get_phi(sim,step,folder):\n",
    "    return cv2.imread(f\"{folder}/Image-{sim}-{step}_phi.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get pressure\n",
    "def get_pres(sim,step,folder):\n",
    "    return cv2.imread(f\"{folder}/Image-{sim}-{step}_P.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get conductivity K\n",
    "def get_k(sim,step,folder):\n",
    "    return cv2.imread(f\"{folder}/Image-{sim}-{step}_K.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get all 3 as a 3-channel matrix\n",
    "def get_all(sim,step,folder):\n",
    "    return np.array((get_k(sim,step,folder), get_pres(sim,step,folder), get_phi(sim,step,folder)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8db3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123); np.random.seed(123)\n",
    "\n",
    "# Pre-define steps and points to maintain a consistent validation set\n",
    "val_steps = np.random.randint(1,199,(val_sims.shape[0],))\n",
    "val_points = np.random.randint(0,149,(val_sims.shape[0],2))\n",
    "val_type = np.random.randint(0,2,(val_sims.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6210558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darcy loss function\n",
    "def darcy_loss(model, inp):\n",
    "    # Takes in the k,pres,phi and outputs the prediction across the image.\n",
    "    inp = inp.requires_grad_(True)\n",
    "    out = model(inp)\n",
    "    # out is in order K,P,phi, (conductivity, pressure, porosity)\n",
    "\n",
    "    # Impose high pressure along the entire upper line by setting the pressure channelt to 200.\n",
    "    out[:, 1:2, 0, :] = 200\n",
    "\n",
    "    # If we assume the output is in order k,pres,phi\n",
    "    # pres_grad is the gradient of the pressure along the y and x directions as a tuple\n",
    "    pres_grad = torch.gradient(out[:, 1:2], dim=(-2,-1))\n",
    "\n",
    "    # get velocity by multiplying the gradient by the conductivity\n",
    "    y_grad = pres_grad[0] * out[:, 0:1]\n",
    "    x_grad = pres_grad[1] * out[:, 0:1]\n",
    "\n",
    "    # compute the divergence by the second derivative of the gradients and adding them together\n",
    "    yy_grad = torch.gradient(y_grad, spacing=(1,),dim=(-2,))[0]\n",
    "    xx_grad = torch.gradient(x_grad, spacing=(1,),dim=(-1,))[0]\n",
    "    final = yy_grad + xx_grad\n",
    "\n",
    "    # total divergence should be 0\n",
    "    loss = (final**2).mean()\n",
    "\n",
    "    return loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Blocks of the Unet\n",
    "\n",
    "class TwoConv(nn.Module):\n",
    "    # Basic block with 2 convolutional layers, each with a batch norm and relu\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, no_end_relu=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if no_end_relu:\n",
    "            self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.seq(inp)\n",
    "\n",
    "# A single conv layer that will increase the height and width of the matrix by 2 each.\n",
    "class SmallUp(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return F.relu(self.conv(inp))\n",
    "\n",
    "# A single conv layer that will decrease the height and width of the matrix by 2 each.\n",
    "class SmallDown(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 0)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return F.relu(self.conv(inp))\n",
    "    \n",
    "# Applies two convolutional layers, then pools\n",
    "class Downsample(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = TwoConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        return self.pool(self.conv(inp))\n",
    "\n",
    "# Upsamples and concatenates the upsampled matrix with the \"across\" then performs convolution on the result\n",
    "class Upsample(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, tweak=None):\n",
    "        super().__init__()\n",
    "        # Upsamples by 2x\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.tweak = tweak\n",
    "        self.conv_after = TwoConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, below, across):\n",
    "        # First upsample by 2x\n",
    "        upsampled = self.up(below)\n",
    "        # If tweak is active, apply it first\n",
    "        if not self.tweak == None:\n",
    "            upsampled = self.tweak(upsampled)\n",
    "        # Concatenate with the same size on the downswing of the unet\n",
    "        concat = torch.concat((upsampled, across), dim=-3)\n",
    "        # Convolute them together\n",
    "        return self.conv_after(concat)\n",
    "    \n",
    "# Define the actual model used\n",
    "class SmallUnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input is Nx3x200x200\n",
    "        self.c1 = TwoConv(3, 8)\n",
    "        self.d1 = Downsample(8,16) # 16x100x100\n",
    "        self.d2 = Downsample(16,32) # 32x50x50\n",
    "        self.su = nn.Sequential(\n",
    "            SmallUp(32),\n",
    "            SmallUp(32),\n",
    "            SmallUp(32)\n",
    "        ) # 3x56x56\n",
    "        self.d3 = Downsample(32,64) # 64x28x28\n",
    "        self.d4 = Downsample(64,128) # 128x14x14\n",
    "        self.d5 = Downsample(128, 256) # 256x7x7\n",
    "\n",
    "        # Now back up\n",
    "        self.u1 = Upsample(256, 128) # 128x14x14\n",
    "        self.u2 = Upsample(128, 64) # 64x28x28\n",
    "        self.u3 = Upsample(64, 32, tweak=nn.Sequential(\n",
    "            SmallDown(32),\n",
    "            SmallDown(32),\n",
    "            SmallDown(32)\n",
    "        ))  # 32x50x50\n",
    "        self.u4 = Upsample(32,16) # 16x100x100\n",
    "        self.u5 = Upsample(16,8) # 8x200x200\n",
    "        self.final = TwoConv(8, 3, no_end_relu=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Start with convolution, expand 3 channels to 8.\n",
    "        # Then downsample 5 times, saving the result\n",
    "        top = self.c1(input)\n",
    "        x1 = self.d1(top)\n",
    "        x2 = self.d2(x1)\n",
    "        x3 = self.d3(self.su(x2)) # Here we upsample slightly so that we can downsample with less border artifacts\n",
    "        x4 = self.d4(x3)\n",
    "        x5 = self.d5(x4)\n",
    "        # Now that we're at 256x7x7, we upsample from here.\n",
    "        # At each layer with concatenate with the xi that is the same size as the up after upsampling.\n",
    "        up = self.u1(x5, x4)\n",
    "        up = self.u2(up, x3)\n",
    "        up = self.u3(up, x2) # Again, a small downsample here to get back on the proper resolution\n",
    "        up = self.u4(up, x1)\n",
    "        up = self.u5(up, top)\n",
    "        # One last convolution on the result to return to 3 channels from 8, leaving us with the proper 3x200x200\n",
    "        return self.final(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e969716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used\n",
    "class MaskedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 sims,\n",
    "                 unmask_size=20,\n",
    "                 points = None,\n",
    "                 block_size = 50,\n",
    "                 reveal_strategy = \"block\",\n",
    "                 n_points = 200,\n",
    "                 radius = 2,\n",
    "                 steps = None,\n",
    "                 H=200,\n",
    "                 W=200,\n",
    "                 channels=\"all\",\n",
    "                 mixed=False,\n",
    "                 types=None,\n",
    "                 noise=5,\n",
    "                 return_mask=False,                 # allows visualiztion of mask\n",
    "                 reveal_dim=[[(0, 1)], [(0, 1)]],   # x,y range for disks to exist\n",
    "                 jitter_std=0.0,                    # % each disk drifts from deterministic position\n",
    "                 deterministic_mask=True            # if True, mask is deterministic and noise is 0. True for testing\n",
    "                 ):\n",
    "        \n",
    "        self.sims = sims\n",
    "        self.points = points\n",
    "        self.steps = steps\n",
    "        self.size = unmask_size\n",
    "        self.reveal_strategy = reveal_strategy\n",
    "        self.block_size = block_size\n",
    "        self.n_points = n_points\n",
    "        self.radius = radius\n",
    "        self.H, self.W = H, W\n",
    "        self.channels = channels\n",
    "        self.mixed = mixed\n",
    "        self.types = types\n",
    "        self.noise = noise\n",
    "        self.return_mask = return_mask\n",
    "        self.reveal_dim = reveal_dim\n",
    "        self.jitter_std = jitter_std\n",
    "        self.deterministic_mask = deterministic_mask\n",
    "\n",
    "    def _chan_idx(self):\n",
    "        if self.channels == \"all\":\n",
    "            return [0,1,2]\n",
    "        elif self.channels == \"K\":\n",
    "            return [0]\n",
    "        elif self.channels == \"P\":\n",
    "            return [1]\n",
    "        elif self.channels == \"phi\":\n",
    "            return [2]\n",
    "        else:\n",
    "            raise ValueError(\"channels must be 'all', 'K', 'P', or 'phi'\")\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        chans = self._chan_idx()\n",
    "\n",
    "\n",
    "        if not type(self.steps) == np.ndarray:\n",
    "            step = np.random.randint(1,200)\n",
    "        else:\n",
    "            step = self.steps[index]\n",
    "\n",
    "        if self.mixed:\n",
    "            if not type(self.types) == np.ndarray:\n",
    "                kind = np.random.randint(0,2)\n",
    "            else:\n",
    "                kind = self.types[index]\n",
    "        else:\n",
    "            kind = 0\n",
    "        if kind == 0:\n",
    "            folder = BINARY_FOLDER\n",
    "        else:\n",
    "            folder = UNIFORM_FOLDER\n",
    "\n",
    "        # Create tensor for the target\n",
    "        t = torch.tensor(get_all(self.sims[index], step, folder))\n",
    "\n",
    "        # Create 0-matrix\n",
    "        z = torch.zeros_like(t)\n",
    "\n",
    "        # build a boolean mask of revealed pixels, shape (H,W)\n",
    "        mask = torch.zeros((self.H, self.W), dtype=torch.bool)\n",
    "\n",
    "        if self.reveal_strategy == \"block\":\n",
    "            # choose top-left for the block\n",
    "            if not type(self.points) == np.ndarray:\n",
    "                i0 = np.random.randint(0, self.H - self.block_size + 1)\n",
    "                j0 = np.random.randint(0, self.W - self.block_size + 1)\n",
    "            else:\n",
    "                i0, j0 = self.points[index]\n",
    "                i0 = max(0, min(i0, self.H - self.block_size))\n",
    "                j0 = max(0, min(j0, self.W - self.block_size))\n",
    "            mask[i0:i0+self.block_size, j0:j0+self.block_size] = True\n",
    "\n",
    "\n",
    "            # new disk mask method\n",
    "            # deterministic disk strategy\n",
    "        elif self.reveal_strategy == \"disks\":\n",
    "\n",
    "            # used for reveal_dim\n",
    "            # map fraction [0,1] to pixel indices [0, N-1] in mask layer\n",
    "            def _segments_to_indices(segments, N, pad=0):\n",
    "                idxs = []\n",
    "                for a, b in segments:\n",
    "                    i0 = max(pad, int(round(a * (N - 1))))\n",
    "                    i1 = min((N - 1) - pad, int(round(b * (N - 1))))\n",
    "                    if i1 >= i0:\n",
    "                        idxs.append(torch.arange(i0, i1 + 1, dtype=torch.long))\n",
    "                if not idxs:\n",
    "                    # fallback to full range\n",
    "                    return torch.arange(pad, N - pad, dtype=torch.long)\n",
    "                return torch.unique(torch.cat(idxs)).to(torch.long)\n",
    "\n",
    "            row_fracs = self.reveal_dim[0] # e.g, [(0, 1)]\n",
    "            col_fracs = self.reveal_dim[1] # e.g, [(0, 1)]\n",
    "            row_allowed = _segments_to_indices(row_fracs, self.H, pad=self.radius)\n",
    "            col_allowed = _segments_to_indices(col_fracs, self.W, pad=self.radius)\n",
    "\n",
    "            # choose grid shape close to aspect ratio \n",
    "            # works with non-squares\n",
    "            Hspan = (row_allowed[-1] - row_allowed[0] + 1) if len(row_allowed) > 0 else self.H\n",
    "            Wspan = (col_allowed[-1] - col_allowed[0] + 1) if len(col_allowed) > 0 else self.W\n",
    "            ratio = float(Wspan) / max(1.0, float(Hspan))\n",
    "            ny = int(max(1, round(np.sqrt(self.n_points / max(1e-8, ratio)))))\n",
    "            nx = int(max(1, round(self.n_points / ny)))\n",
    "            while nx * ny < self.n_points:\n",
    "                nx += 1\n",
    "\n",
    "            # pick evenly spaced indices from rows/cols allowed\n",
    "            def pick_lin_indices(allowed, k):\n",
    "                if k <= 1:\n",
    "                    return allowed[len(allowed)//2]\n",
    "                pos = torch.linspace(0, len(allowed)-1, steps=k)\n",
    "                idx = torch.round(pos).long()\n",
    "                return allowed[idx]\n",
    "            \n",
    "            \n",
    "            row_picks = pick_lin_indices(row_allowed, ny)\n",
    "            col_picks = pick_lin_indices(col_allowed, nx)\n",
    "            yy, xx = torch.meshgrid(row_picks, col_picks, indexing=\"ij\")\n",
    "            points = torch.stack([yy.reshape(-1), xx.reshape(-1)], dim=1) # (ny*nx, 2)\n",
    "            \n",
    "            # if more than n_points, subselect\n",
    "            if points.shape[0] > self.n_points:\n",
    "                sel_pos = torch.linspace(0, points.shape[0]-1, steps=self.n_points)\n",
    "                sel_idx = torch.round(sel_pos).long()\n",
    "                points = points[sel_idx]\n",
    "\n",
    "            ii = points[:, 0]\n",
    "            jj = points[:, 1]\n",
    "\n",
    "            if not self.deterministic_mask:\n",
    "                if self.jitter_std is not None and self.jitter_std > 0:\n",
    "                    # convert std (like 0.01 of image size) to pixels\n",
    "                    sigmaH = float(self.jitter_std) * self.H\n",
    "                    sigmaW = float(self.jitter_std) * self.W\n",
    "                    \n",
    "                    # Add Gaussian noise in pixel units\n",
    "                    ii = ii.to(torch.float32) + torch.randn_like(ii, dtype=torch.float32) * sigmaH\n",
    "                    jj = jj.to(torch.float32) + torch.randn_like(jj, dtype=torch.float32) * sigmaW\n",
    "\n",
    "                    # Round and clamp so they stay inside bounds\n",
    "                    ii = ii.round().clamp(self.radius, self.H - 1 - self.radius).to(torch.long)\n",
    "                    jj = jj.round().clamp(self.radius, self.W - 1 - self.radius).to(torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            yy, xx = torch.meshgrid(torch.arange(self.H), torch.arange(self.W), indexing=\"ij\")\n",
    "            for y0, x0 in zip(ii, jj):\n",
    "                disk = (yy - int(y0))**2 + (xx - int(x0))**2 <= (self.radius**2)\n",
    "                mask |= disk\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reveal_strategy: {self.reveal_strategy}\")\n",
    "        \n",
    "\n",
    "        if not self.deterministic_mask: # only when non deterministic\n",
    "            obs = t[chans].clone()\n",
    "            # Add Gausian noise (0 - 255 scale)\n",
    "            if self.noise is not None and self.noise > 0:\n",
    "                sigma = float(self.noise)\n",
    "                obs = obs + sigma * torch.randn_like(obs)\n",
    "                obs.clamp_(0.0, 255.0)\n",
    "            t = obs[chans].clone()\n",
    "    \n",
    "        \n",
    "\n",
    "        # write revealed pixels for selected channels\n",
    "        chans = self._chan_idx()\n",
    "        z[chans, :, :] = torch.where(mask, t[chans, :, :], torch.zeros_like(t[chans, :, :]))\n",
    "\n",
    "        if self.return_mask:\n",
    "            return z,t, mask\n",
    "        else:  \n",
    "            return z,t\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sims.shape[0]\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(MaskedDataset(train_sims,\n",
    "                                                         reveal_strategy=\"disks\",\n",
    "                                                         n_points=12,\n",
    "                                                         radius=5,\n",
    "                                                         mixed=True),\n",
    "                                                         batch_size=8, shuffle=True)\n",
    "# Val uses preset points and steps\n",
    "val_data = MaskedDataset(\n",
    "    val_sims,\n",
    "    reveal_strategy=\"disks\",\n",
    "    n_points=12,\n",
    "    radius=5,\n",
    "    points=val_points,\n",
    "    steps=val_steps,\n",
    "    mixed=True,\n",
    "    types=val_type\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bafd6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optim, schedule, crit, epochs=250, name = \"test\"):\n",
    "    # train loss values, losses is total darcy is only darcy\n",
    "    losses, darcy = [], []\n",
    "    # similar for validation\n",
    "    val_loss, val_darcy = [], []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        epoch_darcy = 0\n",
    "        for feat,label in train_loader:\n",
    "            optim.zero_grad()\n",
    "            feat = feat.to(device)\n",
    "            label = label.to(device)\n",
    "            # Process darcy loss and save it\n",
    "            p_loss, out = darcy_loss(model, feat)\n",
    "            epoch_darcy += p_loss.item()\n",
    "            # Calculate total loss\n",
    "            loss = p_loss + crit(out, label)\n",
    "            epoch_loss += loss.item()\n",
    "            # Perform backward step\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        # Track loss\n",
    "        epoch_loss /= train_loader.__len__()\n",
    "        epoch_darcy /= train_loader.__len__()\n",
    "        losses.append(epoch_loss)\n",
    "        darcy.append(epoch_darcy)\n",
    "\n",
    "        schedule.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_darcy = 0\n",
    "        with torch.no_grad():\n",
    "            for feat,label in val_loader:\n",
    "\n",
    "                feat = feat.to(device)\n",
    "                label = label.to(device)\n",
    "                p_loss, out = darcy_loss(model, feat)\n",
    "                epoch_darcy += p_loss.item()\n",
    "                loss = p_loss + crit(out, label)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        epoch_loss /= val_loader.__len__()\n",
    "        epoch_darcy /= val_loader.__len__()\n",
    "        val_loss.append(epoch_loss)\n",
    "        val_darcy.append(epoch_darcy)\n",
    "\n",
    "    torch.save(model, f\"{name}.pt\")\n",
    "\n",
    "\n",
    "    \n",
    "    # saving \"curves_{name}.npz\"\n",
    "    np.savez(\n",
    "    f\"curves_{name}.npz\",\n",
    "    train_total=np.array(losses),\n",
    "    train_darcy=np.array(darcy),\n",
    "    val_total=np.array(val_loss),\n",
    "    val_darcy=np.array(val_darcy)\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"train_total\": losses,\n",
    "        \"train_darcy\": darcy,\n",
    "        \"val_total\": val_loss,\n",
    "        \"val_darcy\": val_darcy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a45b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0: r fixed, vary n_points (≈ increasing coverage)\n",
    "tests = [\n",
    "    {\"name\":\"cov_r5_n6_noise0_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":6 , \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n12_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n20_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":20, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n30_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":30, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "\n",
    "# A1: n_points fixed, vary r (≈ increasing coverage)\n",
    "tests += [\n",
    "    {\"name\":\"cov_n12_r3_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":3, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r5_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r7_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":7, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r9_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":9, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "# B0: keep (n=12, r=5) fixed; sweep noise\n",
    "tests += [\n",
    "    {\"name\":\"noise_n12_r5_n0_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n5_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n10_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":10, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n15_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":15, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "# C0: moderate coverage; sweep jitter_std (as % of image side)\n",
    "tests += [\n",
    "    {\"name\":\"jitt_n12_r5_j0_DF\"  , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.00},\n",
    "    {\"name\":\"jitt_n12_r5_j1p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.01},\n",
    "    {\"name\":\"jitt_n12_r5_j2p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.02},\n",
    "    {\"name\":\"jitt_n12_r5_j4p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.04},\n",
    "]\n",
    "\n",
    "# D0: all area vs center-only vs border-only\n",
    "tests += [\n",
    "    {\"name\":\"layout_full_DT\"  , \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \"reveal_dim\":[[(0,1)],[(0,1)]]},  # full image\n",
    "\n",
    "    {\"name\":\"layout_center_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \"reveal_dim\":[[(0.33,0.66)],[(0.33,0.66)]]},  # middle square\n",
    "\n",
    "    {\"name\":\"layout_border_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \n",
    "     \"reveal_dim\":[[(0,0.15),(0.85,1)],[(0,1)]]},  # top/bottom bands\n",
    "]\n",
    "\n",
    "# E0: target ~similar coverage\n",
    "# (n=8, r=7) vs (n=16, r=5) vs (n=32, r=3)\n",
    "tests += [\n",
    "    {\"name\":\"design_n8_r7_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":8 , \"radius\":7, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"design_n16_r5_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"design_n32_r3_DT\", \"reveal_strategy\":\"disks\", \"n_points\":32, \"radius\":3, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "#{\"name\":\"channels_P_only\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"noise\":5, \"deterministic_mask\":True, \"channels\":\"P\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e78d0b",
   "metadata": {},
   "source": [
    "Trains each configuration listed in `{tests}` one at a time.  \n",
    "For every model trained, these files are saved:\n",
    "- **`curves_{name}.npz`** – training and validation loss curves  \n",
    "- **`disks_{name}.pt`** – trained model weights  \n",
    "- **`meta_{name}.npz`** – run metadata (setup, noise)\n",
    "\n",
    "These outputs are used for evaluation and comparison in **`plot.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c22d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m crit = nn.MSELoss()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# --- train (your train() already saves curves_{name}.npz and {name}.pt) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m results[config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = {\u001b[33m\"\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m\"\u001b[39m: hist, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model.state_dict()}\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- save meta so the plotting notebook can reconstruct val_data exactly ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, val_loader, model, optim, schedule, crit, epochs, name)\u001b[39m\n\u001b[32m     13\u001b[39m label = label.to(device)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Process darcy loss and save it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m p_loss, out = \u001b[43mdarcy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m epoch_darcy += p_loss.item()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Calculate total loss\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mdarcy_loss\u001b[39m\u001b[34m(model, inp)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdarcy_loss\u001b[39m(model, inp):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Takes in the k,pres,phi and outputs the prediction across the image.\u001b[39;00m\n\u001b[32m      4\u001b[39m     inp = inp.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# out is in order K,P,phi, (conductivity, pressure, porosity)\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Impose high pressure along the entire upper line by setting the pressure channelt to 200.\u001b[39;00m\n\u001b[32m      9\u001b[39m     out[:, \u001b[32m1\u001b[39m:\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, :] = \u001b[32m200\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mSmallUnet.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    119\u001b[39m top = \u001b[38;5;28mself\u001b[39m.c1(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    120\u001b[39m x1 = \u001b[38;5;28mself\u001b[39m.d1(top)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m x2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m x3 = \u001b[38;5;28mself\u001b[39m.d3(\u001b[38;5;28mself\u001b[39m.su(x2)) \u001b[38;5;66;03m# Here we upsample slightly so that we can downsample with less border artifacts\u001b[39;00m\n\u001b[32m    123\u001b[39m x4 = \u001b[38;5;28mself\u001b[39m.d4(x3)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mDownsample.forward\u001b[39m\u001b[34m(self, inp)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mTwoConv.forward\u001b[39m\u001b[34m(self, inp)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for config in tests:\n",
    "    # read optional extras with defaults\n",
    "    deterministic_mask = config.get(\"deterministic_mask\", False)   # allow jitter during TRAIN by default\n",
    "    jitter_std         = float(config.get(\"jitter_std\", 0.02))     # e.g., 2% grid jitter for training\n",
    "    reveal_dim         = config.get(\"reveal_dim\", [[(0,1)], [(0,1)]])\n",
    "    channels           = config.get(\"channels\", \"all\")\n",
    "    mixed              = bool(config.get(\"mixed\", True))            # <- use local var\n",
    "\n",
    "    # --- TRAIN dataset/loader (can be non-deterministic to augment) ---\n",
    "    train_data = MaskedDataset(\n",
    "        train_sims,\n",
    "        reveal_strategy=config[\"reveal_strategy\"],\n",
    "        n_points=config[\"n_points\"],\n",
    "        radius=config[\"radius\"],\n",
    "        mixed=mixed,                          # <- use local var\n",
    "        noise=config[\"noise\"],\n",
    "        channels=channels,\n",
    "        reveal_dim=reveal_dim,\n",
    "        deterministic_mask=deterministic_mask,\n",
    "        jitter_std=jitter_std\n",
    "        # types=None  # training can randomize if you want\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "\n",
    "    # --- VAL dataset/loader (keep deterministic & no jitter for fair comparison) ---\n",
    "    val_data = MaskedDataset(\n",
    "        val_sims,\n",
    "        reveal_strategy=config[\"reveal_strategy\"],\n",
    "        n_points=config[\"n_points\"],\n",
    "        radius=config[\"radius\"],\n",
    "        mixed=mixed,\n",
    "        noise=config[\"noise\"],\n",
    "        channels=channels,\n",
    "        points=val_points,\n",
    "        steps=val_steps,\n",
    "        reveal_dim=reveal_dim,\n",
    "        deterministic_mask=True,               # fixed evaluation mask\n",
    "        jitter_std=0.0                         # no jitter at eval\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=8)\n",
    "\n",
    "    # --- model/optim/schedule ---\n",
    "    model = SmallUnet().to(device)\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    schedule = torch.optim.lr_scheduler.ExponentialLR(optim, 0.99)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    # --- train (your train() already saves curves_{name}.npz and {name}.pt) ---\n",
    "    hist = train(train_loader, val_loader, model, optim, schedule, crit, epochs=250, name=config[\"name\"])\n",
    "    results[config[\"name\"]] = {\"hist\": hist, \"model\": model.state_dict()}\n",
    "\n",
    "    # --- save meta so the plotting notebook can reconstruct val_data exactly ---\n",
    "    np.savez(\n",
    "        f\"meta_{config['name']}.npz\",\n",
    "        reveal_strategy=np.array(config[\"reveal_strategy\"]),\n",
    "        n_points=np.array(config[\"n_points\"]),\n",
    "        radius=np.array(config[\"radius\"]),\n",
    "        noise=np.array(config[\"noise\"]),\n",
    "        channels=np.array(channels),\n",
    "        # validation slices:\n",
    "        val_steps=val_steps,\n",
    "        val_points=val_points,\n",
    "        # mask/grid settings:\n",
    "        reveal_dim=np.array(reveal_dim, dtype=object),  # load with allow_pickle=True\n",
    "        deterministic_mask_train=np.array(deterministic_mask),\n",
    "        jitter_std_train=np.array(jitter_std),\n",
    "        deterministic_mask_val=np.array(True),\n",
    "        jitter_std_val=np.array(0.0),\n",
    "        mixed=np.array(mixed),\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
