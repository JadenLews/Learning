{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b325813",
   "metadata": {},
   "source": [
    "Same code from PINN we have been working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bf807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Prevents crashes when showing graphs\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e5b24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Determined train/test/val split\n",
    "train_sims = np.load(\"../train_sims.npy\")\n",
    "train_sims = train_sims[train_sims < 750]\n",
    "val_sims = np.load(\"../val_sims.npy\")\n",
    "val_sims = val_sims[val_sims < 750]\n",
    "test_sims = np.load(\"../test_sims.npy\")\n",
    "test_sims = test_sims[test_sims < 750]\n",
    "\n",
    "# Get porosity phi\n",
    "def get_phi(sim,step):\n",
    "    return cv2.imread(f\"../Data200x200_withinfo/Image-{sim}-{step}_phi.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get pressure\n",
    "def get_pres(sim,step):\n",
    "    return cv2.imread(f\"../Data200x200_withinfo/Image-{sim}-{step}_P.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get conductivity K\n",
    "def get_k(sim,step):\n",
    "    return cv2.imread(f\"../Data200x200_withinfo/Image-{sim}-{step}_K.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get all 3 as a 3-channel matrix\n",
    "def get_all(sim,step):\n",
    "    return np.array((get_k(sim,step), get_pres(sim,step), get_phi(sim,step)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8db3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-define steps and points to maintain a consistent validation set\n",
    "val_steps = np.random.randint(1,199,(val_sims.shape[0],))\n",
    "val_points = np.random.randint(0,149,(val_sims.shape[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6210558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darcy loss function\n",
    "def darcy_loss(model, inp):\n",
    "    # Takes in the k,pres,phi and outputs the prediction across the image.\n",
    "    inp = inp.requires_grad_(True)\n",
    "    out = model(inp)\n",
    "    # out is in order K,P,phi, (conductivity, pressure, porosity)\n",
    "\n",
    "    # Impose high pressure along the entire upper line by setting the pressure channelt to 200.\n",
    "    out[:, 1:2, 0, :] = 200\n",
    "\n",
    "    # If we assume the output is in order k,pres,phi\n",
    "    # pres_grad is the gradient of the pressure along the y and x directions as a tuple\n",
    "    pres_grad = torch.gradient(out[:, 1:2], dim=(-2,-1))\n",
    "\n",
    "    # get velocity by multiplying the gradient by the conductivity\n",
    "    y_grad = pres_grad[0] * out[:, 0:1]\n",
    "    x_grad = pres_grad[1] * out[:, 0:1]\n",
    "\n",
    "    # compute the divergence by the second derivative of the gradients and adding them together\n",
    "    yy_grad = torch.gradient(y_grad, spacing=(1,),dim=(-2,))[0]\n",
    "    xx_grad = torch.gradient(x_grad, spacing=(1,),dim=(-1,))[0]\n",
    "    final = yy_grad + xx_grad\n",
    "\n",
    "    # total divergence should be 0\n",
    "    loss = (final**2).mean()\n",
    "\n",
    "    return loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Blocks of the Unet\n",
    "\n",
    "class TwoConv(nn.Module):\n",
    "    # Basic block with 2 convolutional layers, each with a batch norm and relu\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, no_end_relu=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if no_end_relu:\n",
    "            self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.seq(inp)\n",
    "\n",
    "# A single conv layer that will increase the height and width of the matrix by 2 each.\n",
    "class SmallUp(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return F.relu(self.conv(inp))\n",
    "\n",
    "# A single conv layer that will decrease the height and width of the matrix by 2 each.\n",
    "class SmallDown(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 0)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return F.relu(self.conv(inp))\n",
    "    \n",
    "# Applies two convolutional layers, then pools\n",
    "class Downsample(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = TwoConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        return self.pool(self.conv(inp))\n",
    "\n",
    "# Upsamples and concatenates the upsampled matrix with the \"across\" then performs convolution on the result\n",
    "class Upsample(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, tweak=None):\n",
    "        super().__init__()\n",
    "        # Upsamples by 2x\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.tweak = tweak\n",
    "        self.conv_after = TwoConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, below, across):\n",
    "        # First upsample by 2x\n",
    "        upsampled = self.up(below)\n",
    "        # If tweak is active, apply it first\n",
    "        if not self.tweak == None:\n",
    "            upsampled = self.tweak(upsampled)\n",
    "        # Concatenate with the same size on the downswing of the unet\n",
    "        concat = torch.concat((upsampled, across), dim=-3)\n",
    "        # Convolute them together\n",
    "        return self.conv_after(concat)\n",
    "    \n",
    "# Define the actual model used\n",
    "class SmallUnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input is Nx3x200x200\n",
    "        self.c1 = TwoConv(3, 8)\n",
    "        self.d1 = Downsample(8,16) # 16x100x100\n",
    "        self.d2 = Downsample(16,32) # 32x50x50\n",
    "        self.su = nn.Sequential(\n",
    "            SmallUp(32),\n",
    "            SmallUp(32),\n",
    "            SmallUp(32)\n",
    "        ) # 3x56x56\n",
    "        self.d3 = Downsample(32,64) # 64x28x28\n",
    "        self.d4 = Downsample(64,128) # 128x14x14\n",
    "        self.d5 = Downsample(128, 256) # 256x7x7\n",
    "\n",
    "        # Now back up\n",
    "        self.u1 = Upsample(256, 128) # 128x14x14\n",
    "        self.u2 = Upsample(128, 64) # 64x28x28\n",
    "        self.u3 = Upsample(64, 32, tweak=nn.Sequential(\n",
    "            SmallDown(32),\n",
    "            SmallDown(32),\n",
    "            SmallDown(32)\n",
    "        ))  # 32x50x50\n",
    "        self.u4 = Upsample(32,16) # 16x100x100\n",
    "        self.u5 = Upsample(16,8) # 8x200x200\n",
    "        self.final = TwoConv(8, 3, no_end_relu=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Start with convolution, expand 3 channels to 8.\n",
    "        # Then downsample 5 times, saving the result\n",
    "        top = self.c1(input)\n",
    "        x1 = self.d1(top)\n",
    "        x2 = self.d2(x1)\n",
    "        x3 = self.d3(self.su(x2)) # Here we upsample slightly so that we can downsample with less border artifacts\n",
    "        x4 = self.d4(x3)\n",
    "        x5 = self.d5(x4)\n",
    "        # Now that we're at 256x7x7, we upsample from here.\n",
    "        # At each layer with concatenate with the xi that is the same size as the up after upsampling.\n",
    "        up = self.u1(x5, x4)\n",
    "        up = self.u2(up, x3)\n",
    "        up = self.u3(up, x2) # Again, a small downsample here to get back on the proper resolution\n",
    "        up = self.u4(up, x1)\n",
    "        up = self.u5(up, top)\n",
    "        # One last convolution on the result to return to 3 channels from 8, leaving us with the proper 3x200x200\n",
    "        return self.final(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e969716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used\n",
    "class MaskedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 sims,\n",
    "                 unmask_size=20,\n",
    "                 points = None,\n",
    "                 block_size = 50,\n",
    "                 reveal_strategy = \"block\",\n",
    "                 n_points = 200,\n",
    "                 radius = 2,\n",
    "                 steps = None,\n",
    "                 H=200,\n",
    "                 W=200,\n",
    "                 channels=\"all\",\n",
    "                 mixed=False,\n",
    "                 types=None,\n",
    "                 noise=5,\n",
    "                 return_mask=False,                 # allows visualiztion of mask\n",
    "                 reveal_dim=[[(0, 1)], [(0, 1)]],   # x,y range for disks to exist\n",
    "                 jitter_std=0.0,                    # % each disk drifts from deterministic position\n",
    "                 deterministic_mask=True            # if True, mask is deterministic and noise is 0. True for testing\n",
    "                 ):\n",
    "        \n",
    "        self.sims = sims\n",
    "        self.points = points\n",
    "        self.steps = steps\n",
    "        self.size = unmask_size\n",
    "        self.reveal_strategy = reveal_strategy\n",
    "        self.block_size = block_size\n",
    "        self.n_points = n_points\n",
    "        self.radius = radius\n",
    "        self.H, self.W = H, W\n",
    "        self.channels = channels\n",
    "        self.mixed = mixed\n",
    "        self.types = types\n",
    "        self.noise = noise\n",
    "        self.return_mask = return_mask\n",
    "        self.reveal_dim = reveal_dim\n",
    "        self.jitter_std = jitter_std\n",
    "        self.deterministic_mask = deterministic_mask\n",
    "\n",
    "    def _chan_idx(self):\n",
    "        if self.channels == \"all\":\n",
    "            return [0,1,2]\n",
    "        elif self.channels == \"K\":\n",
    "            return [0]\n",
    "        elif self.channels == \"P\":\n",
    "            return [1]\n",
    "        elif self.channels == \"phi\":\n",
    "            return [2]\n",
    "        else:\n",
    "            raise ValueError(\"channels must be 'all', 'K', 'P', or 'phi'\")\n",
    "        \n",
    "\n",
    "    def _segments_to_indices(self, segments, N, pad=0):\n",
    "        idxs = []\n",
    "        for a, b in segments:\n",
    "            # map fraction [0,1] to pixel indices [0, N-1]\n",
    "            i0 = max(pad, int(round(a * (N - 1))))\n",
    "            i1 = min((N - 1) - pad, int(round(b * (N - 1))))\n",
    "            if i1 >= i0:\n",
    "                idxs.append(torch.arange(i0, i1 + 1, dtype=torch.long))\n",
    "        if not idxs:\n",
    "            # fallback to full range if nothing provided\n",
    "            return torch.arange(pad, N - pad, dtype=torch.long)\n",
    "        return torch.unique(torch.cat(idxs)).to(torch.long)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not type(self.steps) == np.ndarray:\n",
    "            step = np.random.randint(1,200)\n",
    "        else:\n",
    "            step = self.steps[index]\n",
    "\n",
    "        # Create tensor for the target\n",
    "        t = torch.tensor(get_all(self.sims[index], step), dtype=torch.float32)\n",
    "\n",
    "        # Create 0 matrix\n",
    "        z = torch.zeros_like(t)\n",
    "\n",
    "        # build a boolean mask of revealed pixels, shape (H,W)\n",
    "        mask = torch.zeros((self.H, self.W), dtype=torch.bool)\n",
    "\n",
    "        chans = self._chan_idx()\n",
    "\n",
    "        if self.reveal_strategy == \"block\":\n",
    "            # choose top-left for the block\n",
    "            if not type(self.points) == np.ndarray:\n",
    "                i0 = np.random.randint(0, self.H - self.block_size + 1)\n",
    "                j0 = np.random.randint(0, self.W - self.block_size + 1)\n",
    "            else:\n",
    "                i0, j0 = self.points[index]\n",
    "                i0 = max(0, min(i0, self.H - self.block_size))\n",
    "                j0 = max(0, min(j0, self.W - self.block_size))\n",
    "            mask[i0:i0+self.block_size, j0:j0+self.block_size] = True\n",
    "\n",
    "        elif self.reveal_strategy == \"disks\":\n",
    "            row_fracs = self.reveal_dim[0] # e.g, [(0, 1)]\n",
    "            col_fracs = self.reveal_dim[1]\n",
    "\n",
    "            row_allowed = self._segments_to_indices(row_fracs, self.H, pad=self.radius)\n",
    "            col_allowed = self._segments_to_indices(col_fracs, self.W, pad=self.radius)\n",
    "\n",
    "            # choose grid shape close to aspect ratio works with non-squares\n",
    "            Hspan = (row_allowed[-1] - row_allowed[0] + 1) if len(row_allowed) > 0 else self.H\n",
    "            Wspan = (col_allowed[-1] - col_allowed[0] + 1) if len(col_allowed) > 0 else self.W\n",
    "            ratio = float(Wspan) / max(1.0, float(Hspan))\n",
    "\n",
    "            ny = int(max(1, round(np.sqrt(self.n_points / max(1e-8, ratio)))))\n",
    "            nx = int(max(1, round(self.n_points / ny)))\n",
    "\n",
    "            print(nx, ny)\n",
    "\n",
    "            while nx * ny < self.n_points:\n",
    "                nx += 1\n",
    "\n",
    "            # pick evenly spaced indices from rows/cols allowed\n",
    "            def pick_lin_indices(allowed, k):\n",
    "                if k <= 1:\n",
    "                    return allowed[len(allowed)//2]\n",
    "                # linespace over positions\n",
    "                pos = torch.linspace(0, len(allowed)-1, steps=k)\n",
    "                idx = torch.round(pos).long()\n",
    "                return allowed[idx]\n",
    "            \n",
    "            row_picks = pick_lin_indices(row_allowed, ny)\n",
    "            col_picks = pick_lin_indices(col_allowed, nx)\n",
    "\n",
    "            print(col_picks, row_picks)\n",
    "\n",
    "            yy, xx = torch.meshgrid(row_picks, col_picks, indexing=\"ij\")\n",
    "            points = torch.stack([yy.reshape(-1), xx.reshape(-1)], dim=1) # (ny*nx, 2)\n",
    "            \n",
    "            # if more than n_points, subselect\n",
    "            if points.shape[0] > self.n_points:\n",
    "                sel_pos = torch.linspace(0, points.shape[0]-1, steps=self.n_points)\n",
    "                sel_idx = torch.round(sel_pos).long()\n",
    "                points = points[sel_idx]\n",
    "\n",
    "            ii = points[:, 0]\n",
    "            jj = points[:, 1]\n",
    "\n",
    "\n",
    "            yy, xx = torch.meshgrid(torch.arange(self.H), torch.arange(self.W), indexing=\"ij\")\n",
    "            for y0, x0 in zip(ii, jj):\n",
    "                disk = (yy - int(y0))**2 + (xx - int(x0))**2 <= (self.radius**2)\n",
    "                mask |= disk\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reveal_strategy: {self.reveal_strategy}\")\n",
    "        \n",
    "\n",
    "        obs = t[chans].clone()\n",
    "        # Add Gausian noise (0 - 255 scale)\n",
    "        if self.noise is not None and self.noise > 0:\n",
    "            sigma = float(self.noise)\n",
    "            obs = obs + sigma * torch.randn_like(obs)\n",
    "            obs.clamp_(0.0, 255.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # write revealed pixels for selected channels\n",
    "        z[chans, :, :] = torch.where(mask, obs, torch.zeros_like(obs))\n",
    "\n",
    "        if self.return_mask:\n",
    "            return z,t, mask\n",
    "        else:  \n",
    "            return z,t\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sims.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ed2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optim, schedule, crit, epochs=250, name = \"test\"):\n",
    "    # train loss values, losses is total darcy is only darcy\n",
    "    losses, darcy = [], []\n",
    "    # similar for validation\n",
    "    val_loss, val_darcy = [], []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        epoch_darcy = 0\n",
    "        for feat,label in train_loader:\n",
    "            optim.zero_grad()\n",
    "            feat = feat.to(device)\n",
    "            label = label.to(device)\n",
    "            # Process darcy loss and save it\n",
    "            p_loss, out = darcy_loss(model, feat)\n",
    "            epoch_darcy += p_loss.item()\n",
    "            # Calculate total loss\n",
    "            loss = p_loss + crit(out, label)\n",
    "            epoch_loss += loss.item()\n",
    "            # Perform backward step\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        # Track loss\n",
    "        epoch_loss /= train_loader.__len__()\n",
    "        epoch_darcy /= train_loader.__len__()\n",
    "        losses.append(epoch_loss)\n",
    "        darcy.append(epoch_darcy)\n",
    "\n",
    "        schedule.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_darcy = 0\n",
    "        with torch.no_grad():\n",
    "            for feat,label in val_loader:\n",
    "\n",
    "                feat = feat.to(device)\n",
    "                label = label.to(device)\n",
    "                p_loss, out = darcy_loss(model, feat)\n",
    "                epoch_darcy += p_loss.item()\n",
    "                loss = p_loss + crit(out, label)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        epoch_loss /= val_loader.__len__()\n",
    "        epoch_darcy /= val_loader.__len__()\n",
    "        val_loss.append(epoch_loss)\n",
    "        val_darcy.append(epoch_darcy)\n",
    "\n",
    "    torch.save(model, f\"{name}.pt\")\n",
    "\n",
    "\n",
    "    \n",
    "    # saving \"curves_{name}.npz\"\n",
    "    np.savez(\n",
    "    f\"curves_{name}.npz\",\n",
    "    train_total=np.array(losses),\n",
    "    train_darcy=np.array(darcy),\n",
    "    val_total=np.array(val_loss),\n",
    "    val_darcy=np.array(val_darcy)\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"train_total\": losses,\n",
    "        \"train_darcy\": darcy,\n",
    "        \"val_total\": val_loss,\n",
    "        \"val_darcy\": val_darcy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a124114",
   "metadata": {},
   "source": [
    "NEW:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7eb2f5",
   "metadata": {},
   "source": [
    "Here you can add tests to run, each one takes a whole train test cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c5a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0: r fixed, vary n_points (≈ increasing coverage)\n",
    "tests = [\n",
    "    {\"name\":\"cov_r5_n6_noise0_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":6 , \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n12_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n20_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":20, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n30_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":30, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "\n",
    "# A1: n_points fixed, vary r (≈ increasing coverage)\n",
    "tests += [\n",
    "    {\"name\":\"cov_n12_r3_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":3, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r5_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r7_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":7, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r9_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":9, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "# B0: keep (n=12, r=5) fixed; sweep noise\n",
    "tests += [\n",
    "    {\"name\":\"noise_n12_r5_n0_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n5_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n10_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":10, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n15_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":15, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "# C0: moderate coverage; sweep jitter_std (as % of image side)\n",
    "tests += [\n",
    "    {\"name\":\"jitt_n12_r5_j0_DF\"  , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.00},\n",
    "    {\"name\":\"jitt_n12_r5_j1p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.01},\n",
    "    {\"name\":\"jitt_n12_r5_j2p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.02},\n",
    "    {\"name\":\"jitt_n12_r5_j4p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.04},\n",
    "]\n",
    "\n",
    "# D0: all area vs center-only vs border-only\n",
    "tests += [\n",
    "    {\"name\":\"layout_full_DT\"  , \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \"reveal_dim\":[[(0,1)],[(0,1)]]},  # full image\n",
    "\n",
    "    {\"name\":\"layout_center_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \"reveal_dim\":[[(0.33,0.66)],[(0.33,0.66)]]},  # middle square\n",
    "\n",
    "    {\"name\":\"layout_border_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \n",
    "     \"reveal_dim\":[[(0,0.15),(0.85,1)],[(0,1)]]},  # top/bottom bands\n",
    "]\n",
    "\n",
    "# E0: target ~similar coverage\n",
    "# (n=8, r=7) vs (n=16, r=5) vs (n=32, r=3)\n",
    "tests += [\n",
    "    {\"name\":\"design_n8_r7_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":8 , \"radius\":7, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"design_n16_r5_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"design_n32_r3_DT\", \"reveal_strategy\":\"disks\", \"n_points\":32, \"radius\":3, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "#{\"name\":\"channels_P_only\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"noise\":5, \"deterministic_mask\":True, \"channels\":\"P\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3e6b6",
   "metadata": {},
   "source": [
    "Trains each configuration listed in `{tests}` one at a time.  \n",
    "For every model trained, these files are saved:\n",
    "- **`curves_{name}.npz`** – training and validation loss curves  \n",
    "- **`disks_{name}.pt`** – trained model weights  \n",
    "- **`meta_{name}.npz`** – run metadata (setup, noise)\n",
    "\n",
    "These outputs are used for evaluation and comparison in **`plot.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31aa1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [03:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m crit = nn.MSELoss()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# --- train (your train() already saves curves_{name}.npz and {name}.pt) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m results[config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = {\u001b[33m\"\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m\"\u001b[39m: hist, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model.state_dict()}\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- save meta so the plotting notebook can reconstruct val_data exactly ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, val_loader, model, optim, schedule, crit, epochs, name)\u001b[39m\n\u001b[32m     19\u001b[39m     epoch_loss += loss.item()\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Perform backward step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     optim.step()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Track loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for config in tests:\n",
    "    # read optional extras with defaults\n",
    "    deterministic_mask = config.get(\"deterministic_mask\", False)   # allow jitter during TRAIN by default\n",
    "    jitter_std         = float(config.get(\"jitter_std\", 0.02))     # e.g., 2% grid jitter for training\n",
    "    reveal_dim         = config.get(\"reveal_dim\", [[(0,1)], [(0,1)]])\n",
    "    channels           = config.get(\"channels\", \"all\")\n",
    "    mixed              = bool(config.get(\"mixed\", True))            # <- use local var\n",
    "\n",
    "    # --- TRAIN dataset/loader (can be non-deterministic to augment) ---\n",
    "    train_data = MaskedDataset(\n",
    "        train_sims,\n",
    "        reveal_strategy=config[\"reveal_strategy\"],\n",
    "        n_points=config[\"n_points\"],\n",
    "        radius=config[\"radius\"],\n",
    "        mixed=mixed,                          # <- use local var\n",
    "        noise=config[\"noise\"],\n",
    "        channels=channels,\n",
    "        reveal_dim=reveal_dim,\n",
    "        deterministic_mask=deterministic_mask,\n",
    "        jitter_std=jitter_std\n",
    "        # types=None  # training can randomize if you want\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "\n",
    "    # --- VAL dataset/loader (keep deterministic & no jitter for fair comparison) ---\n",
    "    val_data = MaskedDataset(\n",
    "        val_sims,\n",
    "        reveal_strategy=config[\"reveal_strategy\"],\n",
    "        n_points=config[\"n_points\"],\n",
    "        radius=config[\"radius\"],\n",
    "        mixed=mixed,\n",
    "        noise=config[\"noise\"],\n",
    "        channels=channels,\n",
    "        points=val_points,\n",
    "        steps=val_steps,\n",
    "        reveal_dim=reveal_dim,\n",
    "        deterministic_mask=True,               # fixed evaluation mask\n",
    "        jitter_std=0.0                         # no jitter at eval\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=8)\n",
    "\n",
    "    # --- model/optim/schedule ---\n",
    "    model = SmallUnet().to(device)\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    schedule = torch.optim.lr_scheduler.ExponentialLR(optim, 0.99)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    # --- train (your train() already saves curves_{name}.npz and {name}.pt) ---\n",
    "    hist = train(train_loader, val_loader, model, optim, schedule, crit, epochs=250, name=config[\"name\"])\n",
    "    results[config[\"name\"]] = {\"hist\": hist, \"model\": model.state_dict()}\n",
    "\n",
    "    # --- save meta so the plotting notebook can reconstruct val_data exactly ---\n",
    "    np.savez(\n",
    "        f\"meta_{config['name']}.npz\",\n",
    "        reveal_strategy=np.array(config[\"reveal_strategy\"]),\n",
    "        n_points=np.array(config[\"n_points\"]),\n",
    "        radius=np.array(config[\"radius\"]),\n",
    "        noise=np.array(config[\"noise\"]),\n",
    "        channels=np.array(channels),\n",
    "        # validation slices:\n",
    "        val_steps=val_steps,\n",
    "        val_points=val_points,\n",
    "        # mask/grid settings:\n",
    "        reveal_dim=np.array(reveal_dim, dtype=object),  # load with allow_pickle=True\n",
    "        deterministic_mask_train=np.array(deterministic_mask),\n",
    "        jitter_std_train=np.array(jitter_std),\n",
    "        deterministic_mask_val=np.array(True),\n",
    "        jitter_std_val=np.array(0.0),\n",
    "        mixed=np.array(mixed),\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 5\n",
      "tensor([  1,  40,  80, 119, 159, 198]) tensor([  1,  50,  99, 149, 198])\n",
      "torch.Size([3, 200, 200]) torch.Size([3, 200, 200]) torch.Size([200, 200])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the dataset (as you did)\n",
    "val_data = MaskedDataset(\n",
    "    val_sims,\n",
    "    reveal_strategy=\"disks\",\n",
    "    n_points=30,\n",
    "    radius=1,\n",
    "    noise=0,\n",
    "    reveal_dim=[[(0, 1)], [(0, 1)]],   # full image for both dims\n",
    "    return_mask=True,\n",
    ")\n",
    "\n",
    "z, t, mask = val_data[0]\n",
    "print(z.shape, t.shape, mask.shape)  # expect (3,200,200), (3,200,200), (200,200)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask.numpy(), cmap=\"gray\")\n",
    "plt.title(\"Revealed-pixels mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e574ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
