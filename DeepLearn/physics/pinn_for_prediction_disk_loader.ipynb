{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b325813",
   "metadata": {},
   "source": [
    "Same code from PINN we have been working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bf807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Prevents crashes when showing graphs\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e5b24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Determined train/test/val split\n",
    "train_sims = np.load(\"../train_sims.npy\")\n",
    "train_sims = train_sims[train_sims < 750]\n",
    "val_sims = np.load(\"../val_sims.npy\")\n",
    "val_sims = val_sims[val_sims < 750]\n",
    "test_sims = np.load(\"../test_sims.npy\")\n",
    "test_sims = test_sims[test_sims < 750]\n",
    "\n",
    "# Get porosity phi\n",
    "def get_phi(sim,step):\n",
    "    return cv2.imread(f\"../Data200x200_withinfo/Image-{sim}-{step}_phi.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get pressure\n",
    "def get_pres(sim,step):\n",
    "    return cv2.imread(f\"../Data200x200_withinfo/Image-{sim}-{step}_P.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get conductivity K\n",
    "def get_k(sim,step):\n",
    "    return cv2.imread(f\"../Data200x200_withinfo/Image-{sim}-{step}_K.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get all 3 as a 3-channel matrix\n",
    "def get_all(sim,step):\n",
    "    return np.array((get_k(sim,step), get_pres(sim,step), get_phi(sim,step)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8db3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-define steps and points to maintain a consistent validation set\n",
    "val_steps = np.random.randint(1,199,(val_sims.shape[0],))\n",
    "val_points = np.random.randint(0,149,(val_sims.shape[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6210558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darcy loss function\n",
    "def darcy_loss(model, inp):\n",
    "    # Takes in the k,pres,phi and outputs the prediction across the image.\n",
    "    inp = inp.requires_grad_(True)\n",
    "    out = model(inp)\n",
    "    # out is in order K,P,phi, (conductivity, pressure, porosity)\n",
    "\n",
    "    # Impose high pressure along the entire upper line by setting the pressure channelt to 200.\n",
    "    out[:, 1:2, 0, :] = 200\n",
    "\n",
    "    # If we assume the output is in order k,pres,phi\n",
    "    # pres_grad is the gradient of the pressure along the y and x directions as a tuple\n",
    "    pres_grad = torch.gradient(out[:, 1:2], dim=(-2,-1))\n",
    "\n",
    "    # get velocity by multiplying the gradient by the conductivity\n",
    "    y_grad = pres_grad[0] * out[:, 0:1]\n",
    "    x_grad = pres_grad[1] * out[:, 0:1]\n",
    "\n",
    "    # compute the divergence by the second derivative of the gradients and adding them together\n",
    "    yy_grad = torch.gradient(y_grad, spacing=(1,),dim=(-2,))[0]\n",
    "    xx_grad = torch.gradient(x_grad, spacing=(1,),dim=(-1,))[0]\n",
    "    final = yy_grad + xx_grad\n",
    "\n",
    "    # total divergence should be 0\n",
    "    loss = (final**2).mean()\n",
    "\n",
    "    return loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Blocks of the Unet\n",
    "\n",
    "class TwoConv(nn.Module):\n",
    "    # Basic block with 2 convolutional layers, each with a batch norm and relu\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, no_end_relu=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if no_end_relu:\n",
    "            self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.seq(inp)\n",
    "\n",
    "# A single conv layer that will increase the height and width of the matrix by 2 each.\n",
    "class SmallUp(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return F.relu(self.conv(inp))\n",
    "\n",
    "# A single conv layer that will decrease the height and width of the matrix by 2 each.\n",
    "class SmallDown(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 0)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return F.relu(self.conv(inp))\n",
    "    \n",
    "# Applies two convolutional layers, then pools\n",
    "class Downsample(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = TwoConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        return self.pool(self.conv(inp))\n",
    "\n",
    "# Upsamples and concatenates the upsampled matrix with the \"across\" then performs convolution on the result\n",
    "class Upsample(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, tweak=None):\n",
    "        super().__init__()\n",
    "        # Upsamples by 2x\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.tweak = tweak\n",
    "        self.conv_after = TwoConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, below, across):\n",
    "        # First upsample by 2x\n",
    "        upsampled = self.up(below)\n",
    "        # If tweak is active, apply it first\n",
    "        if not self.tweak == None:\n",
    "            upsampled = self.tweak(upsampled)\n",
    "        # Concatenate with the same size on the downswing of the unet\n",
    "        concat = torch.concat((upsampled, across), dim=-3)\n",
    "        # Convolute them together\n",
    "        return self.conv_after(concat)\n",
    "    \n",
    "# Define the actual model used\n",
    "class SmallUnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input is Nx3x200x200\n",
    "        self.c1 = TwoConv(3, 8)\n",
    "        self.d1 = Downsample(8,16) # 16x100x100\n",
    "        self.d2 = Downsample(16,32) # 32x50x50\n",
    "        self.su = nn.Sequential(\n",
    "            SmallUp(32),\n",
    "            SmallUp(32),\n",
    "            SmallUp(32)\n",
    "        ) # 3x56x56\n",
    "        self.d3 = Downsample(32,64) # 64x28x28\n",
    "        self.d4 = Downsample(64,128) # 128x14x14\n",
    "        self.d5 = Downsample(128, 256) # 256x7x7\n",
    "\n",
    "        # Now back up\n",
    "        self.u1 = Upsample(256, 128) # 128x14x14\n",
    "        self.u2 = Upsample(128, 64) # 64x28x28\n",
    "        self.u3 = Upsample(64, 32, tweak=nn.Sequential(\n",
    "            SmallDown(32),\n",
    "            SmallDown(32),\n",
    "            SmallDown(32)\n",
    "        ))  # 32x50x50\n",
    "        self.u4 = Upsample(32,16) # 16x100x100\n",
    "        self.u5 = Upsample(16,8) # 8x200x200\n",
    "        self.final = TwoConv(8, 3, no_end_relu=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Start with convolution, expand 3 channels to 8.\n",
    "        # Then downsample 5 times, saving the result\n",
    "        top = self.c1(input)\n",
    "        x1 = self.d1(top)\n",
    "        x2 = self.d2(x1)\n",
    "        x3 = self.d3(self.su(x2)) # Here we upsample slightly so that we can downsample with less border artifacts\n",
    "        x4 = self.d4(x3)\n",
    "        x5 = self.d5(x4)\n",
    "        # Now that we're at 256x7x7, we upsample from here.\n",
    "        # At each layer with concatenate with the xi that is the same size as the up after upsampling.\n",
    "        up = self.u1(x5, x4)\n",
    "        up = self.u2(up, x3)\n",
    "        up = self.u3(up, x2) # Again, a small downsample here to get back on the proper resolution\n",
    "        up = self.u4(up, x1)\n",
    "        up = self.u5(up, top)\n",
    "        # One last convolution on the result to return to 3 channels from 8, leaving us with the proper 3x200x200\n",
    "        return self.final(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e969716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used\n",
    "class MaskedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 sims,\n",
    "                 unmask_size=20,\n",
    "                 points = None,\n",
    "                 block_size = 50,\n",
    "                 reveal_strategy = \"block\",\n",
    "                 n_points = 200,\n",
    "                 radius = 2,\n",
    "                 steps = None,\n",
    "                 H=200,\n",
    "                 W=200,\n",
    "                 channels=\"all\",\n",
    "                 mixed=False,\n",
    "                 types=None,\n",
    "                 noise=5,\n",
    "                 return_mask=False,                 # allows visualiztion of mask\n",
    "                 reveal_dim=[[(0, 1)], [(0, 1)]],   # x,y range for disks to exist\n",
    "                 jitter_std=0.0,                    # % each disk drifts from deterministic position\n",
    "                 deterministic_mask=True            # if True, mask is deterministic and noise is 0. True for testing\n",
    "                 ):\n",
    "        \n",
    "        self.sims = sims\n",
    "        self.points = points\n",
    "        self.steps = steps\n",
    "        self.size = unmask_size\n",
    "        self.reveal_strategy = reveal_strategy\n",
    "        self.block_size = block_size\n",
    "        self.n_points = n_points\n",
    "        self.radius = radius\n",
    "        self.H, self.W = H, W\n",
    "        self.channels = channels\n",
    "        self.mixed = mixed\n",
    "        self.types = types\n",
    "        self.noise = noise\n",
    "        self.return_mask = return_mask\n",
    "        self.reveal_dim = reveal_dim\n",
    "        self.jitter_std = jitter_std\n",
    "        self.deterministic_mask = deterministic_mask\n",
    "\n",
    "    def _chan_idx(self):\n",
    "        if self.channels == \"all\":\n",
    "            return [0,1,2]\n",
    "        elif self.channels == \"K\":\n",
    "            return [0]\n",
    "        elif self.channels == \"P\":\n",
    "            return [1]\n",
    "        elif self.channels == \"phi\":\n",
    "            return [2]\n",
    "        else:\n",
    "            raise ValueError(\"channels must be 'all', 'K', 'P', or 'phi'\")\n",
    "        \n",
    "\n",
    "    def _segments_to_indices(self, segments, N, pad=0):\n",
    "        idxs = []\n",
    "        for a, b in segments:\n",
    "            # map fraction [0,1] to pixel indices [0, N-1]\n",
    "            i0 = max(pad, int(round(a * (N - 1))))\n",
    "            i1 = min((N - 1) - pad, int(round(b * (N - 1))))\n",
    "            if i1 >= i0:\n",
    "                idxs.append(torch.arange(i0, i1 + 1, dtype=torch.long))\n",
    "        if not idxs:\n",
    "            # fallback to full range if nothing provided\n",
    "            return torch.arange(pad, N - pad, dtype=torch.long)\n",
    "        return torch.unique(torch.cat(idxs)).to(torch.long)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not type(self.steps) == np.ndarray:\n",
    "            step = np.random.randint(1,200)\n",
    "        else:\n",
    "            step = self.steps[index]\n",
    "\n",
    "        # Create tensor for the target\n",
    "        t = torch.tensor(get_all(self.sims[index], step), dtype=torch.float32)\n",
    "\n",
    "        # Create 0 matrix\n",
    "        z = torch.zeros_like(t)\n",
    "\n",
    "        # build a boolean mask of revealed pixels, shape (H,W)\n",
    "        mask = torch.zeros((self.H, self.W), dtype=torch.bool)\n",
    "\n",
    "        chans = self._chan_idx()\n",
    "\n",
    "        if self.reveal_strategy == \"block\":\n",
    "            # choose top-left for the block\n",
    "            if not type(self.points) == np.ndarray:\n",
    "                i0 = np.random.randint(0, self.H - self.block_size + 1)\n",
    "                j0 = np.random.randint(0, self.W - self.block_size + 1)\n",
    "            else:\n",
    "                i0, j0 = self.points[index]\n",
    "                i0 = max(0, min(i0, self.H - self.block_size))\n",
    "                j0 = max(0, min(j0, self.W - self.block_size))\n",
    "            mask[i0:i0+self.block_size, j0:j0+self.block_size] = True\n",
    "\n",
    "        elif self.reveal_strategy == \"disks\":\n",
    "            row_fracs = self.reveal_dim[0] # e.g, [(0, 1)]\n",
    "            col_fracs = self.reveal_dim[1]\n",
    "\n",
    "            row_allowed = self._segments_to_indices(row_fracs, self.H, pad=self.radius)\n",
    "            col_allowed = self._segments_to_indices(col_fracs, self.W, pad=self.radius)\n",
    "\n",
    "            # choose grid shape close to aspect ratio works with non-squares\n",
    "            Hspan = (row_allowed[-1] - row_allowed[0] + 1) if len(row_allowed) > 0 else self.H\n",
    "            Wspan = (col_allowed[-1] - col_allowed[0] + 1) if len(col_allowed) > 0 else self.W\n",
    "            ratio = float(Wspan) / max(1.0, float(Hspan))\n",
    "\n",
    "            ny = int(max(1, round(np.sqrt(self.n_points / max(1e-8, ratio)))))\n",
    "            nx = int(max(1, round(self.n_points / ny)))\n",
    "\n",
    "            print(nx, ny)\n",
    "\n",
    "            while nx * ny < self.n_points:\n",
    "                nx += 1\n",
    "\n",
    "            # pick evenly spaced indices from rows/cols allowed\n",
    "            def pick_lin_indices(allowed, k):\n",
    "                if k <= 1:\n",
    "                    return allowed[len(allowed)//2]\n",
    "                # linespace over positions\n",
    "                pos = torch.linspace(0, len(allowed)-1, steps=k)\n",
    "                idx = torch.round(pos).long()\n",
    "                return allowed[idx]\n",
    "            \n",
    "            row_picks = pick_lin_indices(row_allowed, ny)\n",
    "            col_picks = pick_lin_indices(col_allowed, nx)\n",
    "\n",
    "            print(col_picks, row_picks)\n",
    "\n",
    "            yy, xx = torch.meshgrid(row_picks, col_picks, indexing=\"ij\")\n",
    "            points = torch.stack([yy.reshape(-1), xx.reshape(-1)], dim=1) # (ny*nx, 2)\n",
    "            \n",
    "            # if more than n_points, subselect\n",
    "            if points.shape[0] > self.n_points:\n",
    "                sel_pos = torch.linspace(0, points.shape[0]-1, steps=self.n_points)\n",
    "                sel_idx = torch.round(sel_pos).long()\n",
    "                points = points[sel_idx]\n",
    "\n",
    "            ii = points[:, 0]\n",
    "            jj = points[:, 1]\n",
    "\n",
    "\n",
    "            yy, xx = torch.meshgrid(torch.arange(self.H), torch.arange(self.W), indexing=\"ij\")\n",
    "            for y0, x0 in zip(ii, jj):\n",
    "                disk = (yy - int(y0))**2 + (xx - int(x0))**2 <= (self.radius**2)\n",
    "                mask |= disk\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reveal_strategy: {self.reveal_strategy}\")\n",
    "        \n",
    "\n",
    "        obs = t[chans].clone()\n",
    "        # Add Gausian noise (0 - 255 scale)\n",
    "        if self.noise is not None and self.noise > 0:\n",
    "            sigma = float(self.noise)\n",
    "            obs = obs + sigma * torch.randn_like(obs)\n",
    "            obs.clamp_(0.0, 255.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # write revealed pixels for selected channels\n",
    "        z[chans, :, :] = torch.where(mask, obs, torch.zeros_like(obs))\n",
    "\n",
    "        if self.return_mask:\n",
    "            return z,t, mask\n",
    "        else:  \n",
    "            return z,t\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sims.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af0ed2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optim, schedule, crit, epochs=250, name = \"test\"):\n",
    "    # train loss values, losses is total darcy is only darcy\n",
    "    losses, darcy = [], []\n",
    "    # similar for validation\n",
    "    val_loss, val_darcy = [], []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        epoch_darcy = 0\n",
    "        for feat,label in train_loader:\n",
    "            optim.zero_grad()\n",
    "            feat = feat.to(device)\n",
    "            label = label.to(device)\n",
    "            # Process darcy loss and save it\n",
    "            p_loss, out = darcy_loss(model, feat)\n",
    "            epoch_darcy += p_loss.item()\n",
    "            # Calculate total loss\n",
    "            loss = p_loss + crit(out, label)\n",
    "            epoch_loss += loss.item()\n",
    "            # Perform backward step\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        # Track loss\n",
    "        epoch_loss /= train_loader.__len__()\n",
    "        epoch_darcy /= train_loader.__len__()\n",
    "        losses.append(epoch_loss)\n",
    "        darcy.append(epoch_darcy)\n",
    "\n",
    "        schedule.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_darcy = 0\n",
    "        with torch.no_grad():\n",
    "            for feat,label in val_loader:\n",
    "\n",
    "                feat = feat.to(device)\n",
    "                label = label.to(device)\n",
    "                p_loss, out = darcy_loss(model, feat)\n",
    "                epoch_darcy += p_loss.item()\n",
    "                loss = p_loss + crit(out, label)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        epoch_loss /= val_loader.__len__()\n",
    "        epoch_darcy /= val_loader.__len__()\n",
    "        val_loss.append(epoch_loss)\n",
    "        val_darcy.append(epoch_darcy)\n",
    "\n",
    "    torch.save(model, f\"{name}.pt\")\n",
    "\n",
    "\n",
    "    \n",
    "    # saving \"curves_{name}.npz\"\n",
    "    np.savez(\n",
    "    f\"curves_{name}.npz\",\n",
    "    train_total=np.array(losses),\n",
    "    train_darcy=np.array(darcy),\n",
    "    val_total=np.array(val_loss),\n",
    "    val_darcy=np.array(val_darcy)\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"train_total\": losses,\n",
    "        \"train_darcy\": darcy,\n",
    "        \"val_total\": val_loss,\n",
    "        \"val_darcy\": val_darcy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a124114",
   "metadata": {},
   "source": [
    "NEW:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7eb2f5",
   "metadata": {},
   "source": [
    "Here you can add tests to run, each one takes a whole train test cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c5a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0: r fixed, vary n_points (≈ increasing coverage)\n",
    "tests = [\n",
    "    {\"name\":\"cov_r5_n6_noise0_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":6 , \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n12_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n20_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":20, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_r5_n30_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":30, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "\n",
    "# A1: n_points fixed, vary r (≈ increasing coverage)\n",
    "tests += [\n",
    "    {\"name\":\"cov_n12_r3_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":3, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r5_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r7_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":7, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"cov_n12_r9_noise0_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":9, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "# B0: keep (n=12, r=5) fixed; sweep noise\n",
    "tests += [\n",
    "    {\"name\":\"noise_n12_r5_n0_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":0, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n5_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n10_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":10, \"deterministic_mask\":True},\n",
    "    {\"name\":\"noise_n12_r5_n15_DT\", \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":15, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "# C0: moderate coverage; sweep jitter_std (as % of image side)\n",
    "tests += [\n",
    "    {\"name\":\"jitt_n12_r5_j0_DF\"  , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.00},\n",
    "    {\"name\":\"jitt_n12_r5_j1p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.01},\n",
    "    {\"name\":\"jitt_n12_r5_j2p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.02},\n",
    "    {\"name\":\"jitt_n12_r5_j4p_DF\" , \"reveal_strategy\":\"disks\", \"n_points\":12, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":False, \"jitter_std\":0.04},\n",
    "]\n",
    "\n",
    "# D0: all area vs center-only vs border-only\n",
    "tests += [\n",
    "    {\"name\":\"layout_full_DT\"  , \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \"reveal_dim\":[[(0,1)],[(0,1)]]},  # full image\n",
    "\n",
    "    {\"name\":\"layout_center_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \"reveal_dim\":[[(0.33,0.66)],[(0.33,0.66)]]},  # middle square\n",
    "\n",
    "    {\"name\":\"layout_border_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5,\n",
    "     \"deterministic_mask\":True, \n",
    "     \"reveal_dim\":[[(0,0.15),(0.85,1)],[(0,1)]]},  # top/bottom bands\n",
    "]\n",
    "\n",
    "# E0: target ~similar coverage\n",
    "# (n=8, r=7) vs (n=16, r=5) vs (n=32, r=3)\n",
    "tests += [\n",
    "    {\"name\":\"design_n8_r7_DT\" , \"reveal_strategy\":\"disks\", \"n_points\":8 , \"radius\":7, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"design_n16_r5_DT\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "    {\"name\":\"design_n32_r3_DT\", \"reveal_strategy\":\"disks\", \"n_points\":32, \"radius\":3, \"mixed\":True, \"noise\":5, \"deterministic_mask\":True},\n",
    "]\n",
    "\n",
    "#{\"name\":\"channels_P_only\", \"reveal_strategy\":\"disks\", \"n_points\":16, \"radius\":5, \"noise\":5, \"deterministic_mask\":True, \"channels\":\"P\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3e6b6",
   "metadata": {},
   "source": [
    "Trains each configuration listed in `{tests}` one at a time.  \n",
    "For every model trained, these files are saved:\n",
    "- **`curves_{name}.npz`** – training and validation loss curves  \n",
    "- **`disks_{name}.pt`** – trained model weights  \n",
    "- **`meta_{name}.npz`** – run metadata (setup, noise)\n",
    "\n",
    "These outputs are used for evaluation and comparison in **`plot.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31aa1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n",
      "3 2\n",
      "tensor([  5,  99, 194]) tensor([  5, 194])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m crit = nn.MSELoss()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# --- train (your train() already saves curves_{name}.npz and {name}.pt) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m results[config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = {\u001b[33m\"\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m\"\u001b[39m: hist, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model.state_dict()}\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- save meta so the plotting notebook can reconstruct val_data exactly ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, val_loader, model, optim, schedule, crit, epochs, name)\u001b[39m\n\u001b[32m     13\u001b[39m label = label.to(device)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Process darcy loss and save it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m p_loss, out = \u001b[43mdarcy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m epoch_darcy += p_loss.item()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Calculate total loss\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mdarcy_loss\u001b[39m\u001b[34m(model, inp)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdarcy_loss\u001b[39m(model, inp):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Takes in the k,pres,phi and outputs the prediction across the image.\u001b[39;00m\n\u001b[32m      4\u001b[39m     inp = inp.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# out is in order K,P,phi, (conductivity, pressure, porosity)\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Impose high pressure along the entire upper line by setting the pressure channelt to 200.\u001b[39;00m\n\u001b[32m      9\u001b[39m     out[:, \u001b[32m1\u001b[39m:\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, :] = \u001b[32m200\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mSmallUnet.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Start with convolution, expand 3 channels to 8.\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Then downsample 5 times, saving the result\u001b[39;00m\n\u001b[32m    119\u001b[39m     top = \u001b[38;5;28mself\u001b[39m.c1(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     x1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     x2 = \u001b[38;5;28mself\u001b[39m.d2(x1)\n\u001b[32m    122\u001b[39m     x3 = \u001b[38;5;28mself\u001b[39m.d3(\u001b[38;5;28mself\u001b[39m.su(x2)) \u001b[38;5;66;03m# Here we upsample slightly so that we can downsample with less border artifacts\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mDownsample.forward\u001b[39m\u001b[34m(self, inp)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mTwoConv.forward\u001b[39m\u001b[34m(self, inp)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/stochastic/lib/python3.12/site-packages/torch/nn/functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for config in tests:\n",
    "    # read optional extras with defaults\n",
    "    deterministic_mask = config.get(\"deterministic_mask\", False)   # allow jitter during TRAIN by default\n",
    "    jitter_std         = float(config.get(\"jitter_std\", 0.02))     # e.g., 2% grid jitter for training\n",
    "    reveal_dim         = config.get(\"reveal_dim\", [[(0,1)], [(0,1)]])\n",
    "    channels           = config.get(\"channels\", \"all\")\n",
    "    mixed              = bool(config.get(\"mixed\", True))            # <- use local var\n",
    "\n",
    "    # --- TRAIN dataset/loader (can be non-deterministic to augment) ---\n",
    "    train_data = MaskedDataset(\n",
    "        train_sims,\n",
    "        reveal_strategy=config[\"reveal_strategy\"],\n",
    "        n_points=config[\"n_points\"],\n",
    "        radius=config[\"radius\"],\n",
    "        mixed=mixed,                          # <- use local var\n",
    "        noise=config[\"noise\"],\n",
    "        channels=channels,\n",
    "        reveal_dim=reveal_dim,\n",
    "        deterministic_mask=deterministic_mask,\n",
    "        jitter_std=jitter_std\n",
    "        # types=None  # training can randomize if you want\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "\n",
    "    # --- VAL dataset/loader (keep deterministic & no jitter for fair comparison) ---\n",
    "    val_data = MaskedDataset(\n",
    "        val_sims,\n",
    "        reveal_strategy=config[\"reveal_strategy\"],\n",
    "        n_points=config[\"n_points\"],\n",
    "        radius=config[\"radius\"],\n",
    "        mixed=mixed,\n",
    "        noise=config[\"noise\"],\n",
    "        channels=channels,\n",
    "        points=val_points,\n",
    "        steps=val_steps,\n",
    "        reveal_dim=reveal_dim,\n",
    "        deterministic_mask=True,               # fixed evaluation mask\n",
    "        jitter_std=0.0                         # no jitter at eval\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=8)\n",
    "\n",
    "    # --- model/optim/schedule ---\n",
    "    model = SmallUnet().to(device)\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    schedule = torch.optim.lr_scheduler.ExponentialLR(optim, 0.99)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    # --- train (your train() already saves curves_{name}.npz and {name}.pt) ---\n",
    "    hist = train(train_loader, val_loader, model, optim, schedule, crit, epochs=250, name=config[\"name\"])\n",
    "    results[config[\"name\"]] = {\"hist\": hist, \"model\": model.state_dict()}\n",
    "\n",
    "    # --- save meta so the plotting notebook can reconstruct val_data exactly ---\n",
    "    np.savez(\n",
    "        f\"meta_{config['name']}.npz\",\n",
    "        reveal_strategy=np.array(config[\"reveal_strategy\"]),\n",
    "        n_points=np.array(config[\"n_points\"]),\n",
    "        radius=np.array(config[\"radius\"]),\n",
    "        noise=np.array(config[\"noise\"]),\n",
    "        channels=np.array(channels),\n",
    "        # validation slices:\n",
    "        val_steps=val_steps,\n",
    "        val_points=val_points,\n",
    "        # mask/grid settings:\n",
    "        reveal_dim=np.array(reveal_dim, dtype=object),  # load with allow_pickle=True\n",
    "        deterministic_mask_train=np.array(deterministic_mask),\n",
    "        jitter_std_train=np.array(jitter_std),\n",
    "        deterministic_mask_val=np.array(True),\n",
    "        jitter_std_val=np.array(0.0),\n",
    "        mixed=np.array(mixed),\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e22d77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "tensor([  5,  68, 131, 194]) tensor([ 30,  85, 139, 194])\n",
      "torch.Size([3, 200, 200]) torch.Size([3, 200, 200]) torch.Size([200, 200])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the dataset (as you did)\n",
    "val_data = MaskedDataset(\n",
    "    val_sims,\n",
    "    reveal_strategy=\"disks\",\n",
    "    n_points=16,\n",
    "    radius=5,\n",
    "    noise=0,\n",
    "    reveal_dim=[[(0.15,1)],[(0, 1)]],   # full image for both dims\n",
    "    return_mask=True,\n",
    ")\n",
    "\n",
    "z, t, mask = val_data[0]\n",
    "print(z.shape, t.shape, mask.shape)  # expect (3,200,200), (3,200,200), (200,200)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask.numpy(), cmap=\"gray\")\n",
    "plt.title(\"Revealed-pixels mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e574ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
